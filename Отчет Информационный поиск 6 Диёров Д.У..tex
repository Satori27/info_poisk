\documentclass[14pt,a4paper]{extarticle}

\usepackage{geometry}
\geometry{
    left=30mm,
    right=15mm,
    top=20mm,
    bottom=20mm
}

\usepackage{fontspec}
\usepackage{polyglossia}
\setmainlanguage{russian}

\setmainfont{Liberation Serif}
\newfontfamily\cyrillicfont{Liberation Serif}

\setmonofont{Liberation Mono}
\newfontfamily\cyrillicfonttt{Liberation Mono}

\usepackage{setspace}
\onehalfspacing

\usepackage{indentfirst}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage{titlesec}
\usepackage[labelformat=simple, labelsep=none]{caption} 
\renewcommand{\figurename}{Рис.} 

\renewcommand{\figurename}{Рис.} 


\titleformat{\section}
  {\normalfont\bfseries\centering}
  {}
  {0pt}
  {}

\titleformat{\subsection}
  {\normalfont\bfseries}
  {}
  {0pt}
  {}
  

\begin{document}
\begin{titlepage}
\begin{center}
\bfseries

{\Large Московский авиационный институт\\ (национальный исследовательский университет)

}

\vspace{48pt}

{\large Факультет информационных технологий и прикладной математики
}

\vspace{36pt}

{\large Кафедра вычислительной математики и~программирования

}


\vspace{48pt}

Лабораторная работа по курсу \enquote{Информационный поиск}

\end{center}

\vspace{72pt}

\begin{flushright}
\begin{tabular}{rl}
Студент: & Д.\,У. Диёров \\
Преподаватель: & А.\,А. Кухтичев \\
Группа: & М8О-407Б \\
Дата: & 29.12.2025 \\
Оценка: & \\
Подпись: & \\
\end{tabular}
\end{flushright}

\vfill

\begin{center}
\bfseries
Москва, 2025
\end{center}
\end{titlepage}

\pagebreak

\section*{Цель работы}

Найти и проанализировать корпус документов, который будет использован при выполнении последующих лабораторных работ; написать парсер на любом языке программирования; реализовать процесс разбиения текстов документов на токены, который затем будет использоваться при индексации; реализовать стемминг; реализовать булев индекс и булев поиск; написать юнит-тесты для проверки корректности работы.

\section*{Задание}

\subsection*{Парсер}

В качестве корпуса документов был выбран форум \texttt{https://anime-characters-fight.fandom.com}, содержащий около 1\,200\,000 страниц из 23\,000 статей с описаниями персонажей и их способностей. Для скачивания и парсинга страниц был использован скрипт на языке программирования Go.

Скрипт получает начальный набор ссылок и рекурсивно обходит их, проверяя, что статьи принадлежат тому же домену, чтобы избежать парсинга сторонних ресурсов. Для скачивания страниц использовалась стандартная библиотека \texttt{http}, а для парсинга HTML — open-source библиотека 
\url{github.com/PuerkitoBio/goquery}.

Работа парсера проверялась с помощью юнит-тестов. При скачивании структура \texttt{Crawler} проверяет соблюдение правил \texttt{robots.txt}: если ссылка недоступна, она пропускается и не запрашивается повторно.

Метод \texttt{Crawl} скачивает страницу и передаёт её парсеру, который возвращает удобочитаемое JSON-представление. Далее \texttt{Crawler} сохраняет данные в базу данных PostgreSQL. Основная функция скрипта использует паттерн \texttt{WorkerPool} для вызова метода \texttt{Crawl}; между запросами горутины делают паузу в 100 мс, чтобы не создавать чрезмерную нагрузку на форум.

\section*{Производительность токенизации}

Для оценки скорости токенизации был сгенерирован текст объёмом 1024 КБ.

Количество полученных токенов — 149\,798.  
Средняя длина токена — 7{,}0 символов.  
Время токенизации — 0{,}04 секунды.  
Скорость обработки — примерно 25 МБ/с.

Для анализа данных в базе данных был реализован вспомогательный скрипт на Python.

\section*{Качество поиска}

При использовании стемминга были получены следующие усреднённые показатели:

\begin{itemize}
    \item Precision@5 — 0{,}32
    \item Recall@5 — 0{,}80
    \item F1@5 — 0{,}45
\end{itemize}

При отключении стемминга качество поиска улучшилось:

\begin{itemize}
    \item Precision@5 — 0{,}60
    \item Recall@5 — 0{,}80
    \item F1@5 — 0{,}67
\end{itemize}

Полнота осталась на прежнем уровне, однако точность значительно возросла, что говорит о более корректной обработке терминологических и именных запросов без применения стемминга.

\section*{Статистический анализ корпуса}

Для анализа распределения частот токенов были аппроксимированы два закона.

Закон Ципфа показал следующие параметры:
\[
C = 2{,}53 \cdot 10^{4}, \quad s = 0{,}79
\]
Значение параметра $s < 1$ указывает на медленное убывание частот, при котором редкие слова составляют значительную часть словаря.

Закон Мандельброта дал параметры:
\[
C = 2{,}75 \cdot 10^{4}, \quad b = 0{,}137, \quad s = 0{,}81
\]
Добавление параметра смещения $b$ улучшило аппроксимацию распределения, особенно для наиболее частотных слов.

\section*{Токенизация}
Функция tokenize выполняет токенизацию входного текста, то есть разбивает строку на отдельные слова (токены), корректно обрабатывая символы в кодировке UTF-8.
На первом этапе входная строка в формате std::string (UTF-8) преобразуется в широкую строку std::wstring. Это необходимо для корректной работы с Unicode-символами и использования стандартных функций классификации символов (буквы, цифры и т.д.).
Далее устанавливается UTF-8 локаль, что позволяет правильно определять типы символов (буквенно-цифровые или разделители) для многоязычного текста.
Затем функция последовательно проходит по каждому символу широкого текста:
\begin{itemize}
    \item Если символ является буквенно-цифровым (iswalnum), он приводится к нижнему регистру и добавляется к текущему формируемому токену;
    \item Если встречается любой другой символ (пробел, знак пунктуации, спецсимвол), текущий токен считается завершённым и добавляется в результирующий список, после чего начинается формирование нового токена.
\end{itemize}



После обработки всего текста функция дополнительно проверяет, остался ли незавершённый токен, и при необходимости добавляет его в результат.
Каждый сформированный токен перед добавлением в итоговый массив преобразуется обратно из std::wstring в UTF-8 строку (std::string). Результат работы функции сохраняется в БД. Чтение и запись в БД происходит батчами по 5 документов. 


\section*{Булев индекс и булев поиск}

Сначала строковый запрос разбирается функцией \texttt{parse\_bool\_query}. 
Она ищет в строке логические операторы \texttt{AND} или \texttt{OR}, 
разделяет запрос на левый и правый токены и сохраняет тип операции. 
Если оператор не найден, запрос помечается как некорректный.

Для работы с результатами поиска используется структура \texttt{IntArray}, 
которая хранит динамический массив идентификаторов документов и его размер.

Функция \texttt{get\_token\_id} по тексту токена выполняет SQL-запрос к базе данных 
и возвращает его числовой идентификатор. Если токен отсутствует в таблице, возвращается \texttt{-1}.

Функция \texttt{get\_documents\_by\_token} получает из базы данных список идентификаторов документов, 
связанных с заданным токеном, и формирует массив \texttt{IntArray}.

Логические операции над результатами поиска реализованы отдельными функциями: 
\texttt{intersect} вычисляет пересечение двух массивов документов и возвращает только совпадающие идентификаторы;  

\texttt{unite} формирует объединение двух массивов, добавляя элементы без дублирования.

Функция \texttt{boolean\_search} связывает все этапы поиска. Она получает идентификаторы токенов, 
извлекает соответствующие списки документов и в зависимости от операции (\texttt{AND} или \texttt{OR}) 
вызывает пересечение или объединение массивов.

Функция \texttt{boolean\_search\_http} предназначена для использования в HTTP-обработчике. 
Она вызывает булев поиск, а затем по каждому найденному идентификатору документа запрашивает из базы данных 
URL страницы и формирует JSON-ответ со списком найденных ссылок.

\section*{Результаты}

График распределения терминов приведён на рисунке~1.
\begin{figure}[h] 
    \centering 
    \includegraphics[width=15cm,height=7cm]{media/image1.png} 
    \caption{}
    \label{fig:example} 
\end{figure}

Для ввода и вывода запросов был реализован HTTP-сервер на C++, принимающий поисковые запросы и возвращающий результаты.

\section*{Примеры запросов и ответов}
Примеры запросов на рисунке 2 и 3
\begin{figure}[h] 
    \centering 
    \includegraphics[width=15cm,height=7cm]{media/image2.png}
    \caption{} 
    \label{fig:example}
\end{figure}

\begin{figure}[h] 
    \centering 
    \includegraphics[width=15cm,height=7cm]{media/image3.png}
    \caption{} 
    \label{fig:example} 
\end{figure}

\pagebreak
\section*{Вывод}

В ходе лабораторной работы был реализован поисковый индекс с поддержкой токенизации и булевого поиска. Анализ корпуса подтвердил статистические свойства текстовых данных, а реализованный токенизатор продемонстрировал высокую производительность и корректность работы.

\end{document}
